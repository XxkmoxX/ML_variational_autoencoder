{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/venv/cdd/lib/python3.12/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "2025-03-11 15:25:19.215112: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-11 15:25:19.232877: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-11 15:25:19.237925: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-11 15:25:19.251331: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-11 15:25:20.600926: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch                                            # Main deep learning library.\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms            # Handles datasets and transfomations.\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter       # Track training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Variational Autoencoder (VAE) Model \n",
    "\n",
    "### See Conditional VAE that introduces and additional conditioning variable, tipycally lables, allowing for controled generation.\n",
    "\n",
    "'''\n",
    "This class defines the VAE model. It consists of an encoder and a decoder. \n",
    "The encoder compresses the input image into a latent space, and the decoder reconstructs \n",
    "the image from the latent space.\n",
    "'''\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=128):                             # latent_dim=128: The size of latent representation.\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder                                                   # Features extraction: 4 concolutional layers extract hierarchical image features,\n",
    "        self.encoder = nn.Sequential(                               # each layer halves the images size while increasing feature maps.\n",
    "            nn.Conv2d(3, 32, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256 * 16 * 16, latent_dim)           # Latent Space Mwean and Variance: fc_mu and fc_logvar predict mean and          \n",
    "        self.fc_logvar = nn.Linear(256 * 16 * 16, latent_dim)       # log-variance for the latent distribution.\n",
    "\n",
    "        # Decoder                                                   # Image reconstruction: Fulli connected layer (fc_dec) expands latent vector.\n",
    "        self.fc_dec = nn.Linear(latent_dim, 256 * 16 * 16)          # Tranpose convolutions upsample back to 256x256 image.\n",
    "        self.decoder = nn.Sequential(                               # Final layer uses Sigmoid() to constrain pixel values to [0, 1].\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):                           # Reparametrization trick, allows backpropagation the through the \n",
    "        std = torch.exp(0.5 * logvar)                               # stocastic sampling process. Converts mean and log-variance into a latent vactor.\n",
    "        eps = torch.randn_like(std)                                 # Allows backpropagation through a stochastic operation.\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):                                           # Forward pass of the VAE. It encodes the input, samples from latent space, and \n",
    "        x = self.encoder(x).view(x.size(0), -1)                     # decodes the sample back to the image space. 1) Encodes image --> latent vector(mu, logvar).\n",
    "        mu, logvar = self.fc_mu(x), self.fc_logvar(x)               # 2) Samples z using reparametrization trick. 3) Decodes z baxk to an image.\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x = self.fc_dec(z).view(x.size(0), 256, 16, 16)\n",
    "        x = self.decoder(x)\n",
    "        return x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "'''\n",
    "This function trains the VAE. It initializes the model, optimizer, and TensorBoard writer. \n",
    "It then iterates over the dataset for a specified number of epochs, computes the loss, performs \n",
    "backpropagation, and updates the model parameters. The loss is logged to TensorBoard, \n",
    "and the trained model is saved.\n",
    "'''\n",
    "\n",
    "def train_vae(epochs=10, batch_size=32, latent_dim=128):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")           # Uses GPU if avalaible.\n",
    "    dataloader = get_dataloader(batch_size)                                         # Load dataset using DataLoader.\n",
    "    vae = VAE(latent_dim).to(device)                                                # Initializes model and Adam optimizer.\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=1e-4)                               # Creates TensorBoard logger.\n",
    "    writer = SummaryWriter(\"runs/vae_experiment\")                                   \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, mu, logvar = vae(images)\n",
    "            loss = vae_loss(x_recon, images, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader.dataset)                             # Log loss and save model. Save loss TensorBoard and save trained model for later use.\n",
    "        writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    writer.close()\n",
    "    torch.save(vae.state_dict(), \"vae_256x256.pth\")\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and Visualize Samples\n",
    "\n",
    "'''\n",
    "This function generates and visualizes images from the trained VAE. \n",
    "It samples random latent vectors, decodes them into images, and \n",
    "displays the images using Matplotlib.\n",
    "'''\n",
    "\n",
    "def generate_images(vae, num_images=5, latent_dim=128):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vae.to(device)\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_images, latent_dim).to(device)                                  # Samples random latent vectors (z).\n",
    "        generated_images = vae.decoder(vae.fc_dec(z).view(num_images, 256, 16, 16))         # Uses decoder to generate images from z.\n",
    "    \n",
    "    generated_images = generated_images.cpu().numpy().transpose(0, 2, 3, 1)                 # Converts tensor tu NumPy forma for visualization.\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))                               \n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(generated_images[i])\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Training Example\n",
    "# vae = train_vae(epochs=10)\n",
    "# generate_images(vae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Flax JAX implementation\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    latent_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.encoder = nn.Sequential([\n",
    "            nn.Conv(32, (4, 4), strides=2), nn.relu,\n",
    "            nn.Conv(64, (4, 4), strides=2), nn.relu,\n",
    "            nn.Conv(128, (4, 4), strides=2), nn.relu,\n",
    "            nn.Conv(256, (4, 4), strides=2), nn.relu,\n",
    "        ])\n",
    "        self.fc_mu = nn.Dense(self.latent_dim)\n",
    "        self.fc_logvar = nn.Dense(self.latent_dim)\n",
    "        self.fc_dec = nn.Dense(256 * 16 * 16)\n",
    "        self.decoder = nn.Sequential([\n",
    "            nn.ConvTranspose(128, (4, 4), strides=2), nn.relu,\n",
    "            nn.ConvTranspose(64, (4, 4), strides=2), nn.relu,\n",
    "            nn.ConvTranspose(32, (4, 4), strides=2), nn.relu,\n",
    "            nn.ConvTranspose(3, (4, 4), strides=2), nn.sigmoid,\n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        mu, logvar = self.fc_mu(x), self.fc_logvar(x)\n",
    "        std = jnp.exp(0.5 * logvar)\n",
    "        eps = jax.random.normal(jax.random.PRNGKey(0), std.shape)\n",
    "        z = mu + eps * std\n",
    "        x = self.fc_dec(z).reshape((-1, 256, 16, 16))\n",
    "        return self.decoder(x), mu, logvar\n",
    "\n",
    "\n",
    "\n",
    "### TensorFlow (Keras) implementation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Conv2D(32, (4, 4), strides=2, activation='relu'),\n",
    "            layers.Conv2D(64, (4, 4), strides=2, activation='relu'),\n",
    "            layers.Conv2D(128, (4, 4), strides=2, activation='relu'),\n",
    "            layers.Conv2D(256, (4, 4), strides=2, activation='relu'),\n",
    "            layers.Flatten()\n",
    "        ])\n",
    "        self.fc_mu = layers.Dense(latent_dim)\n",
    "        self.fc_logvar = layers.Dense(latent_dim)\n",
    "        self.fc_dec = layers.Dense(256 * 16 * 16)\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(128, (4, 4), strides=2, activation='relu'),\n",
    "            layers.Conv2DTranspose(64, (4, 4), strides=2, activation='relu'),\n",
    "            layers.Conv2DTranspose(32, (4, 4), strides=2, activation='relu'),\n",
    "            layers.Conv2DTranspose(3, (4, 4), strides=2, activation='sigmoid'),\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu, logvar = self.fc_mu(x), self.fc_logvar(x)\n",
    "        std = tf.exp(0.5 * logvar)\n",
    "        eps = tf.random.normal(std.shape)\n",
    "        z = mu + eps * std\n",
    "        x = tf.reshape(self.fc_dec(z), (-1, 256, 16, 16))\n",
    "        return self.decoder(x), mu, logvar\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
